{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Video Classification\n",
    "\n",
    "Module 6 - Video Action Recognition\n",
    "\n",
    "For book, references and training materials, please check this project website [http://activefitness.ai/ai-in-sports-with-python](http://activefitness.ai/ai-in-sports-with-python).\n",
    "\n",
    "Book: [Applied Machine Learning for Health and Fitness](https://www.apress.com/us/book/9781484257715), Chapter 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([255, 720, 1280, 3])\n",
      "torch.Size([2, 407552])\n",
      "{'video_fps': 29.97002997002997, 'audio_fps': 48000}\n"
     ]
    }
   ],
   "source": [
    "import torchvision.io\n",
    "video_file = 'media/surfing_cutback.mp4'\n",
    "video, audio, info = torchvision.io.read_video(video_file, pts_unit='sec')\n",
    "print(video.shape)\n",
    "print(audio.shape)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "athletics - jumping\n",
      "\thigh jump\n",
      "\thurdling\n",
      "\tlong jump\n",
      "\tparkour\n",
      "\tpole vault\n",
      "\ttriple jump\n",
      "athletics - throwing + launching\n",
      "\tarchery\n",
      "\tcatching or throwing frisbee\n",
      "\tdisc golfing\n",
      "\thammer throw\n",
      "\tjavelin throw\n",
      "\tshot put\n",
      "\tthrowing axe\n",
      "\tthrowing ball\n",
      "\tthrowing discus\n",
      "ball sports\n",
      "\tbowling\n",
      "\tcatching or throwing baseball\n",
      "\tcatching or throwing softball\n",
      "\tdodgeball\n",
      "\tdribbling basketball\n",
      "\tdunking basketball\n",
      "\tgolf chipping\n",
      "\tgolf driving\n",
      "\tgolf putting\n",
      "\thitting baseball\n",
      "\thurling (sport)\n",
      "\tjuggling soccer ball\n",
      "\tkicking field goal\n",
      "\tkicking soccer ball\n",
      "\tpassing American football (in game)\n",
      "\tpassing American football (not in game)\n",
      "\tplaying basketball\n",
      "\tplaying cricket\n",
      "\tplaying kickball\n",
      "\tplaying squash or racquetball\n",
      "\tplaying tennis\n",
      "\tplaying volleyball\n",
      "\tshooting basketball\n",
      "\tshooting goal (soccer)\n",
      "\tshot put\n",
      "golf\n",
      "\tgolf chipping\n",
      "\tgolf driving\n",
      "\tgolf putting\n",
      "gymnastics\n",
      "\tbouncing on trampoline\n",
      "\tcartwheeling\n",
      "\tgymnastics tumbling\n",
      "\tsomersaulting\n",
      "\tvault\n",
      "heights\n",
      "\tabseiling\n",
      "\tbungee jumping\n",
      "\tclimbing a rope\n",
      "\tclimbing ladder\n",
      "\tclimbing tree\n",
      "\tdiving cliff\n",
      "\tice climbing\n",
      "\tjumping into pool\n",
      "\tparagliding\n",
      "\trock climbing\n",
      "\tskydiving\n",
      "\tslacklining\n",
      "\tspringboard diving\n",
      "\tswinging on something\n",
      "\ttrapezing\n",
      "martial arts\n",
      "\tarm wrestling\n",
      "\tcapoeira\n",
      "\tdrop kicking\n",
      "\thigh kick\n",
      "\tpunching bag\n",
      "\tpunching person (boxing)\n",
      "\tside kick\n",
      "\tsword fighting\n",
      "\ttai chi\n",
      "\twrestling\n",
      "racquet + bat sports\n",
      "\tcatching or throwing baseball\n",
      "\tcatching or throwing softball\n",
      "\thitting baseball\n",
      "\thurling (sport)\n",
      "\tplaying badminton\n",
      "\tplaying cricket\n",
      "\tplaying squash or racquetball\n",
      "\tplaying tennis\n",
      "snow + ice\n",
      "\tbiking through snow\n",
      "\tbobsledding\n",
      "\thockey stop\n",
      "\tice climbing\n",
      "\tice fishing\n",
      "\tice skating\n",
      "\tmaking snowman\n",
      "\tplaying ice hockey\n",
      "\tshoveling snow\n",
      "\tski jumping\n",
      "\tskiing (not slalom or crosscountry)\n",
      "\tskiing crosscountry\n",
      "\tskiing slalom\n",
      "\tsled dog racing\n",
      "\tsnowboarding\n",
      "\tsnowkiting\n",
      "\tsnowmobiling\n",
      "\ttobogganing\n",
      "swimming\n",
      "\tswimming backstroke\n",
      "\tswimming breast stroke\n",
      "\tswimming butterfly stroke\n",
      "mobility - water\n",
      "\tcrossing river\n",
      "\tdiving cliff\n",
      "\tjumping into pool\n",
      "\tscuba diving\n",
      "\tsnorkeling\n",
      "\tspringboard diving\n",
      "\tswimming backstroke\n",
      "\tswimming breast stroke\n",
      "\tswimming butterfly stroke\n",
      "\twater sliding\n",
      "water sports\n",
      "\tcanoeing or kayaking\n",
      "\tjetskiing\n",
      "\tkitesurfing\n",
      "\tparasailing\n",
      "\tsailing\n",
      "\tsurfing water\n",
      "\twater skiing\n",
      "\twindsurfing\n",
      "custom\n",
      "\tpull ups\n",
      "\tbench pressing\n",
      "\tyoga\n",
      "\tsnatch weight lifting\n",
      "\tfront raises\n",
      "\tsquat\n",
      "\texercising arm\n",
      "\tdeadlifting\n",
      "\tlunge\n",
      "\tsitup\n",
      "\tdoing aerobics\n",
      "\tclean and jerk\n",
      "\tpush up\n",
      "\texercising with an exercise ball\n",
      "Sport activities labels: 134\n"
     ]
    }
   ],
   "source": [
    "from utils.kinetics import kinetics\n",
    "categories = kinetics.categories()\n",
    "classes = kinetics.classes()\n",
    "sports = kinetics.sport_categories()\n",
    "\n",
    "count = 0\n",
    "for key in categories.keys():\n",
    "    if key in sports:\n",
    "        print(key)\n",
    "        for label in categories[key]:\n",
    "            count+=1\n",
    "            print(\"\\t{}\".format(label))\n",
    "print(f'Sport activities labels: {count}')      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models    \n",
    "\n",
    "# check if cuda is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoResNet(\n",
       "  (stem): R2Plus1dStem(\n",
       "    (0): Conv3d(3, 45, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3), bias=False)\n",
       "    (1): BatchNorm3d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv3d(45, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(64, 230, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(230, 128, kernel_size=(3, 1, 1), stride=(2, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(128, 230, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(230, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(128, 288, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(288, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(128, 288, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(288, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(128, 460, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(460, 256, kernel_size=(3, 1, 1), stride=(2, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(256, 460, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(460, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(256, 576, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(576, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(256, 576, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(576, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(256, 921, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(921, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(921, 512, kernel_size=(3, 1, 1), stride=(2, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(512, 921, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(921, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(921, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(512, 1152, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(1152, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(512, 1152, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(1152, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=400, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = models.video.r3d_18(pretrained=True) \n",
    "#model = models.video.mc3_18(pretrained=True) \n",
    "model = models.video.r2plus1d_18(pretrained=True)\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31505325\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models  \n",
    "\n",
    "# Normalization: Kinetics 400\n",
    "mean = [0.43216, 0.394666, 0.37645]  \n",
    "std = [0.22803, 0.22145, 0.216989] \n",
    "\n",
    "def normalize(video): \n",
    "    return video.permute(3, 0, 1, 2).to(torch.float32) / 255\n",
    "\n",
    "def resize(video, size): \n",
    "    return torch.nn.functional.interpolate(video, size=size, scale_factor=None, mode='bilinear', align_corners=False)\n",
    "\n",
    "def crop(video, output_size): \n",
    "    # center crop    \n",
    "    h, w = video.shape[-2:] \n",
    "    th, tw = output_size \n",
    "    i = int(round((h - th) / 2.)) \n",
    "    j = int(round((w - tw) / 2.)) \n",
    "    return video[..., i:(i + th), j:(j + tw)]\n",
    "\n",
    "def normalize_base(video, mean, std): \n",
    "    shape = (-1,) + (1,) * (video.dim() - 1) \n",
    "    mean = torch.as_tensor(mean).reshape(shape) \n",
    "    std = torch.as_tensor(std).reshape(shape) \n",
    "    return (video - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([255, 720, 1280, 3]) torch.Size([2, 407552]) {'video_fps': 29.97002997002997, 'audio_fps': 48000}\n"
     ]
    }
   ],
   "source": [
    "import torchvision.io \n",
    "video_file = 'media/surfing_cutback.mp4'\n",
    "video, audio, info = torchvision.io.read_video(video_file, pts_unit='sec') \n",
    "print(video.shape, audio.shape, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames 3, size 255 112\n"
     ]
    }
   ],
   "source": [
    "video = normalize(video) \n",
    "video = resize(video,(128, 171)) \n",
    "video = crop(video,(112, 112)) \n",
    "video = normalize_base(video, mean=mean, std=std)\n",
    "shape = video.shape\n",
    "print(f'frames {shape[0]}, size {shape[1]} {shape[2]}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.video.r2plus1d_18(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# make use of accelerated CUDA if available\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    video = video.cuda() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the video\n",
    "score = model(video.unsqueeze(0)) \n",
    "# get prediction with max score\n",
    "prediction = score.argmax() \n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.kinetics import kinetics\n",
    "classes = kinetics.classes()\n",
    "print(classes[prediction.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader as DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets.kinetics import Kinetics400\n",
    "from torchvision.datasets.samplers import DistributedSampler, UniformClipSampler, RandomClipSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "Path.ls = lambda x: [o.name for o in x.iterdir()]\n",
    "from torchvision.io.video import read_video\n",
    "from functools import partial as partial\n",
    "read_video = partial(read_video, pts_unit='sec')\n",
    "torchvision.io.read_video = partial(torchvision.io.read_video, pts_unit = 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path('data/kinetics400/')\n",
    "data_dir = base_dir/'dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree {data_dir/'train'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conveniently, as part of torchvision.datasets, PyTorch includes Kinetics400 dataset that serves as a cookie cutter for our project. Internally, video datasets use VideoClips object to store video clips data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 10/10 [00:34<00:00,  3.49s/it]\n"
     ]
    }
   ],
   "source": [
    "data = torchvision.datasets.Kinetics400(\n",
    "            data_dir/'train',\n",
    "            frames_per_clip=32,\n",
    "            step_between_clips=1,\n",
    "            frame_rate=None,\n",
    "            extensions=('mp4',),\n",
    "            num_workers=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Although you can and should take advantage of the multiprocessing nature of datasets, especially in the production environment, on some systems you may get an error, num_workers = 0 makes sure you use dataset single threaded.\n",
    "\n",
    "According to this constructor above, each video clip loaded with our dataset should be a 4D tensor with the shape (frames, height, width, channels), in our case 32 frames, RGB video, note that Kinetics doesn't require all clips to be of the same height/width:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 480, 272, 3])\n"
     ]
    }
   ],
   "source": [
    "print((data[0][0]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing dataset\n",
    "\n",
    "Sometimes, it may be handy to visualize the entire dataset catalog as a table, summarizing the number of frames. The helper function to_dataframe loads the entire video catalog into Pandas DataFrame and displays the content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 35271\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>frames</th>\n",
       "      <th>fps</th>\n",
       "      <th>clips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data\\kinetics400\\dataset\\train\\playing_tennis\\...</td>\n",
       "      <td>300</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data\\kinetics400\\dataset\\train\\playing_tennis\\...</td>\n",
       "      <td>300</td>\n",
       "      <td>29.97003</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data\\kinetics400\\dataset\\train\\playing_tennis\\...</td>\n",
       "      <td>300</td>\n",
       "      <td>29.97003</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data\\kinetics400\\dataset\\train\\playing_tennis\\...</td>\n",
       "      <td>300</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data\\kinetics400\\dataset\\train\\playing_tennis\\...</td>\n",
       "      <td>300</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>data\\kinetics400\\dataset\\train\\surfing_water\\Y...</td>\n",
       "      <td>250</td>\n",
       "      <td>25.00000</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>data\\kinetics400\\dataset\\train\\surfing_water\\Z...</td>\n",
       "      <td>300</td>\n",
       "      <td>29.97003</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>data\\kinetics400\\dataset\\train\\surfing_water\\_...</td>\n",
       "      <td>178</td>\n",
       "      <td>29.97003</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>data\\kinetics400\\dataset\\train\\surfing_water\\a...</td>\n",
       "      <td>119</td>\n",
       "      <td>15.00000</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>data\\kinetics400\\dataset\\train\\surfing_water\\b...</td>\n",
       "      <td>250</td>\n",
       "      <td>25.00000</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              filepath  frames       fps  \\\n",
       "0    data\\kinetics400\\dataset\\train\\playing_tennis\\...     300  30.00000   \n",
       "1    data\\kinetics400\\dataset\\train\\playing_tennis\\...     300  29.97003   \n",
       "2    data\\kinetics400\\dataset\\train\\playing_tennis\\...     300  29.97003   \n",
       "3    data\\kinetics400\\dataset\\train\\playing_tennis\\...     300  30.00000   \n",
       "4    data\\kinetics400\\dataset\\train\\playing_tennis\\...     300  30.00000   \n",
       "..                                                 ...     ...       ...   \n",
       "145  data\\kinetics400\\dataset\\train\\surfing_water\\Y...     250  25.00000   \n",
       "146  data\\kinetics400\\dataset\\train\\surfing_water\\Z...     300  29.97003   \n",
       "147  data\\kinetics400\\dataset\\train\\surfing_water\\_...     178  29.97003   \n",
       "148  data\\kinetics400\\dataset\\train\\surfing_water\\a...     119  15.00000   \n",
       "149  data\\kinetics400\\dataset\\train\\surfing_water\\b...     250  25.00000   \n",
       "\n",
       "     clips  \n",
       "0      269  \n",
       "1      269  \n",
       "2      269  \n",
       "3      269  \n",
       "4      269  \n",
       "..     ...  \n",
       "145    219  \n",
       "146    269  \n",
       "147    147  \n",
       "148     88  \n",
       "149    219  \n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils.video_classification.helpers import to_dataframe\n",
    "\n",
    "to_dataframe(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s say we want to display the size of a video in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 35271\n"
     ]
    }
   ],
   "source": [
    "VIDEO_NUMBER = 130\n",
    "video_table = to_dataframe(data)\n",
    "video_info = video_table['filepath'][VIDEO_NUMBER]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With notebook IPython.display Video helper we can also show the video embedded in the notebook, but keep in mind that setting embed=True while displaying the video may significantly increase the size of your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"data\\kinetics400\\dataset\\train\\surfing_water\\PGHcKxhh5y8.mp4\" controls  width=\"400\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "Video(video_info, width=400, embed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So instead of embedding the video, it may be sufficient to just visualize the first and last frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_clip_start_end(f):\n",
    "    last = len(f)\n",
    "    plt.imshow(f[0])\n",
    "    plt.title(f'frame: 1')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.imshow(f[last-1])\n",
    "    plt.title(f'frame: {last}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_clip_start_end(data[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video normalization\n",
    "\n",
    "As with most of the data, before training our model, video needs to be normalized for video classification models included in torchvision. This involves getting image data in the range \\[0,1\\] and normalizing with standard deviation and the mean provided with the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.video_classification.transforms as T\n",
    "\n",
    "t = torchvision.transforms.Compose([\n",
    "        T.ToFloatTensorInZeroOne(),\n",
    "        T.Resize((128, 171)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.Normalize(mean=[0.43216, 0.394666, 0.37645],\n",
    "                            std=[0.22803, 0.22145, 0.216989]),\n",
    "        T.RandomCrop((112, 112))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've defined the transform, we can pass it to the Kinetics400 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 10/10 [00:39<00:00,  3.97s/it]\n"
     ]
    }
   ],
   "source": [
    "train_data = torchvision.datasets.Kinetics400(\n",
    "            data_dir/'train',\n",
    "            frames_per_clip=32,\n",
    "            step_between_clips=1,\n",
    "            frame_rate=None,\n",
    "            transform=t,\n",
    "            extensions=('mp4',),\n",
    "            num_workers=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader class in PyTorch provides many useful features and makes it easy to use from Python, including: iterable datasets, automatic batching, memory pinning, sampling and data loading order customization etc.\n",
    "\n",
    "## Finding learning rate\n",
    "\n",
    "> *Never let formal education get in the way of your learning.\\\n",
    "> *--Mark Twain\n",
    "\n",
    "Learning rate, as a hyperparameter for training neural networks is important: if you make learning rate too small, the model will likely converge too slowly.\n",
    "\n",
    "**Mysterious constant:** The so-called Karpathy constant defines the best learning rate for Adam as 3e-4. The author of the famous tweet in data science, Andrej himself in the response to his own tweet says that this was a joke. Nevertheless, the constant made it to Urban Dictionary and many data science blogs.\n",
    "\n",
    "We will not take this for granted of course and will use sound theory to find the best learning rate. In practice, a large learning rate may fail to reach model convergence. As an illustration, notice that by making learning rate too large for gradient descent, the model will never reach its minimum.\n",
    "\n",
    "![](images/ch9/fig_9-7.png)\n",
    "\n",
    "To deal with this problem, a paper by Leslie N. Smith [https://arxiv.org/pdf/1506.01186.pdf] was published that proposed a method to optimize finding learning rates. As a result, many frameworks, including fastai and PyTorch now include learning rate finder module. For PyTorch, you can use torch_lr_finder module by installing it with pip install torch-lr-finder and then use it in the code with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_lr_finder import LRFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting dataset ready for learning rate finder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 32, 112, 112]), torch.Size([4]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.video_classification.first_clip_sampler import FirstClipSampler\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # remove audio from the batch\n",
    "    batch = [(d[0], d[2]) for d in batch]\n",
    "    return default_collate(batch)\n",
    "\n",
    "train_sampler = FirstClipSampler(train_data.video_clips, 2)\n",
    "train_dl = DataLoader(train_data, batch_size=4, sampler=train_sampler, collate_fn=collate_fn, pin_memory=True)\n",
    "x,y = next(iter(train_dl))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6, weight_decay=1e-2)\n",
    "# if you are getting memory problems running this, \n",
    "# try reducing DataLoader batch_size above to 16 or even 4\n",
    "lr_finder = LRFinder(model, optimizer, criterion,device=device)\n",
    "lr_finder.range_test(train_dl, end_lr=10, num_iter=90)\n",
    "lr_finder.plot()\n",
    "lr_finder.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of video action recognition, with the size of the data and large differences in training times for video data, it is recommended to use a proper learning rate, which is often in the middle of the descending loss curve. The module plots the loss curve, and the optimal learning rate from the chart below is somewhere near value lr = 1e-2:\n",
    "\n",
    "![](images/ch9/fig_9-8.png)\n",
    "\n",
    "Optimal learning rate is found around the middle of descending loss curve, on this figure around 10\\^-2.\n",
    "\n",
    "## Training the model\n",
    "\n",
    "Training the model for video action recognition in PyTorch follows the same principles as for image classifier, but since video classification functionality is relatively new in PyTorch, it's worth including a small example in this chapter.\n",
    "\n",
    "### Project 9-3. Video Recognition Model Training\n",
    "\n",
    "To start, let's create two datasets, for training and validation, based on built-in Kinetics object. The idea here is to take advantage of built-in objects that PyTorch offers. I use the same normalizing video transformation T, already used in previous examples. On some systems you can get a significant speed improvement if you set num_workers > 0 , but on my system I had to be conservative, so I keep it at zero (basically, it means don't take advantage of parallelization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.Kinetics400(\n",
    "            data_dir/'train',\n",
    "            frames_per_clip=32,\n",
    "            step_between_clips=1,\n",
    "            frame_rate=None,\n",
    "            transform=t,\n",
    "            extensions=('mp4',),\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "valid_data = torchvision.datasets.Kinetics400(\n",
    "            data_dir/'valid',\n",
    "            frames_per_clip=32,\n",
    "            step_between_clips=1,\n",
    "            frame_rate=None,\n",
    "            transform=t,\n",
    "            extensions=('mp4',),\n",
    "            num_workers=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch allows using familiar DataLoaders with video data, and for video data PyTorch includes VideoClips class used for enumerating clips in the video and also sampling clips in the video while loading. FirstClipSampler in the below example used video\\_clips property from the dataset to sample a specified number of clips in the video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = FirstClipSampler(train_data.video_clips, 2)\n",
    "train_dl = DataLoader(train_data, \n",
    "                      batch_size=4, \n",
    "                      sampler=train_sampler, \n",
    "                      collate_fn=collate_fn, \n",
    "                      pin_memory=True)\n",
    "valid_sampler = FirstClipSampler(valid_data.video_clips, 2)\n",
    "valid_dl = DataLoader(valid_data, \n",
    "                      batch_size=4, \n",
    "                      sampler=valid_sampler, \n",
    "                      collate_fn=collate_fn, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and renormalizing video data can take a really long time, so you may want to save the normalized dataset in cache directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cache_dir = data_dir/'.cache'\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "cache_dir.ls()\n",
    "\n",
    "torch.save(train_data, f'{cache_dir}/train')\n",
    "torch.save(valid_data, f'{cache_dir}/valid')\n",
    "train_data = torch.load(cache_dir/'train')\n",
    "valid_data = torch.load(cache_dir/'valid') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you initialize the model with hyper-parameters, including the learning rate obtained earlier. Note that since we'll be training the model, we instantiate it without weights (pretrained=False or omitted):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = models.video.r2plus1d_18()\n",
    "model.cuda()\n",
    "lr = 1e-2\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optim, max_lr=5e-1, steps_per_epoch=len(train_dl), epochs=10)\n",
    "metrics_dir = cache_dir/'train-metrics'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrossEntropyLoss can be used for training classification problems and Adam optimizer (same as we used finding the learning rate). Next, we can train the model, in the example below I chose 10 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "from utils.video_classification.train import train_one_epoch, evaluate\n",
    "\n",
    "start_time = time.time()\n",
    " \n",
    "for epoch in range(10):\n",
    "    train_one_epoch(model, \n",
    "                    criterion, \n",
    "                    optim, \n",
    "                    lr_scheduler, \n",
    "                    train_dl, device, \n",
    "                    epoch, print_freq=100)\n",
    "    evaluate(model, \n",
    "             criterion, \n",
    "             valid_dl, \n",
    "             device)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also save the model weights once it's trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODEL_PATH = './videoresnet_action.pth'\n",
    "torch.save(model.state_dict(), SAVED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this chapter we covered practical methods and tools for video action recognition and classification. We discussed data structures for loading, normalizing and storing videos, datasets for sports action classification, such as Kinetics, and deep learning models. Using readily available pre-trained models, we can classify hundreds of sport actions and train the models to recognize new activities. For a sport data scientist, this chapter provides practical examples for deep learning, movement analysis, action recognition on any video.\n",
    "\n",
    "Although video action recognition is becoming more usable today, and made progress in thousands of classifications, we are still far from the goals of generalized action recognition. That means, as a sport data scientist, you are still left with a lot of work to apply video recognition in the field. Is this the right time to make video action recognition a part of your toolbox? With practical examples and notebooks accompanying this chapter, I think that this is the right time for coaches and sport scientists to start using these methods in everyday sport data science. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
