{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1 Machine Learning in the Cloud\n",
    "\n",
    "Module 8 - AI in the Cloud\n",
    "\n",
    "For book, references and training materials, please check this project website [http://activefitness.ai/ai-in-sports-with-python](http://activefitness.ai/ai-in-sports-with-python).\n",
    "\n",
    "Book: [Applied Machine Learning for Health and Fitness](https://www.apress.com/us/book/9781484257715), Chapters 11-12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.5'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import azureml.core\n",
    "azureml.core.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "workspace = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Workspace, Datastore, Dataset\n",
    "\n",
    "datastore = workspace.get_default_datastore()\n",
    "source_dir = os.getcwd()\n",
    "store_path = 'center_of_mass'\n",
    "\n",
    "datastore.upload_files(\n",
    "    files=[os.path.join(source_dir, f) for f in ['skier_center_of_mass.csv']],\n",
    "    relative_root=source_dir,\n",
    "    target_path=store_path,\n",
    "    overwrite=True)\n",
    "dataset = Dataset.Tabular.from_delimited_files(path=(datastore, store_path))\n",
    "dataset = dataset.register(workspace=workspace,\n",
    "                           name='center_of_mass',\n",
    "                           description='skier center of mass')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling data in the cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial experiment configuration\n",
    "experiment_name = 'activity-classification'\n",
    "script_folder = 'activity-classification'\n",
    "cluster_name = \"compute-experiments\"\n",
    "model_file_name = 'activities.pkl'\n",
    "labeled_dataset_name = 'Classifying activities-2020-03-15 00:54:26'\n",
    "output_folder =  './outputs'\n",
    "local_download_folder = './download/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "from azureml.contrib.dataset import FileHandlingOption\n",
    "\n",
    "dataset = Dataset.get_by_name(workspace, name=labeled_dataset_name)\n",
    "dataset_pd = dataset.to_pandas_dataframe(\n",
    "    file_handling_option=FileHandlingOption.DOWNLOAD, \n",
    "    target_path=local_download_folder, \n",
    "    overwrite_download=True)\n",
    "dataset_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "w=10\n",
    "h=10\n",
    "fig=plt.figure(figsize=(15, 15))\n",
    "plt.subplots_adjust(hspace=0.001)\n",
    "columns = 2\n",
    "rows = 2\n",
    "for i in range(1, columns*rows +1):\n",
    "    img = mpimg.imread(dataset_pd.loc[i+5,'image_url'])\n",
    "    ax = fig.add_subplot(rows, columns, i)\n",
    "    ax.title.set_text(dataset_pd.loc[i+5,'label'])\n",
    "    ax.axis('off')\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as F\n",
    "\n",
    "pytorch_dataset = dataset.to_torchvision()\n",
    "img = pytorch_dataset[0][0]\n",
    "print(type(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing for training\n",
    "======================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'errors': None, 'creationTime': '2020-03-12T14:08:17.927990+00:00', 'createdBy': {'userId': 'e180613e-2ad1-41cc-8aae-8d4183f7b2fd', 'userOrgId': '72f988bf-86f1-41af-91ab-2d7cd011db47'}, 'modifiedTime': '2020-03-12T14:09:04.221524+00:00', 'state': 'Running', 'vmSize': 'STANDARD_D3_V2'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=workspace, name=cluster_name)\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "conda_env = Environment('conda-env')\n",
    "conda_env.python.conda_dependencies = CondaDependencies.create(pip_packages=['azureml-sdk',\n",
    "                                                                             'azureml-contrib-dataset',\n",
    "                                                                             'torch','torchvision',\n",
    "                                                                             'azureml-dataprep[pandas,fuse]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Dataset\n",
    "from azureml.contrib.dataset import FileHandlingOption\n",
    "\n",
    "experiment = Experiment(workspace=workspace, name=experiment_name)\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "dataset = Dataset.get_by_name(workspace, name=labeled_dataset_name)\n",
    "\n",
    "script_params = {\n",
    "    '--output-folder': output_folder,\n",
    "    '--model-file': model_file_name\n",
    "}\n",
    "\n",
    "estimator = Estimator(source_directory=script_folder, \n",
    "                entry_script='train.py',\n",
    "                script_params=script_params,    \n",
    "                inputs=[dataset.as_named_input('activities')],\n",
    "                compute_target=compute_target,\n",
    "                environment_definition=conda_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training in the cloud\n",
    "===========================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset, Run\n",
    "import azureml.contrib.dataset\n",
    "from azureml.contrib.dataset import FileHandlingOption, LabeledDatasetTask\n",
    "\n",
    "run = Run.get_context()\n",
    "# get input dataset by name\n",
    "labeled_dataset = run.input_datasets['activities']\n",
    "\n",
    "mounted_path = tempfile.mkdtemp()\n",
    "# mount dataset onto the mounted_path of a Linux-based compute\n",
    "mount_context = labeled_dataset.mount(mounted_path)\n",
    "mount_context.start()\n",
    "print(os.listdir(mounted_path))\n",
    "print (mounted_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "f = './download/workspaceblobstore/activities'\n",
    "\n",
    "def load(f, size = .2):\n",
    "    \n",
    "    t = transforms.Compose([transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406], \n",
    "        std = [0.229, 0.224, 0.225])])\n",
    "        \n",
    "    train = datasets.ImageFolder(f, transform=t)\n",
    "    test = datasets.ImageFolder(f, transform=t)\n",
    "    n = len(train)\n",
    "    indices = list(range(n))\n",
    "    split = int(np.floor(size * n))\n",
    "    np.random.shuffle(indices)\n",
    "    train_idx, test_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "    trainloader = torch.utils.data.DataLoader(train,sampler=train_sampler, batch_size=64)\n",
    "    testloader = torch.utils.data.DataLoader(test, sampler=test_sampler, batch_size=64)\n",
    "    return trainloader, testloader\n",
    "\n",
    "trainloader, testloader = load(f, .2)\n",
    "print(trainloader.dataset.classes)\n",
    "images, labels = next(iter(trainloader))\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "plt.imshow(grid.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from azureml.core import Dataset, Run\n",
    "import azureml.contrib.dataset\n",
    "from azureml.contrib.dataset import FileHandlingOption, LabeledDatasetTask\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  \n",
    "    \n",
    "run = Run.get_context()\n",
    "\n",
    "# get input dataset by name\n",
    "#labeled_dataset = run.input_datasets['activities']\n",
    "#pytorch_dataset = labeled_dataset.to_torchvision()\n",
    "\n",
    "\n",
    "features = model.fc.in_features\n",
    "model.fc = nn.Linear(features, len(labels))\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "print_every = 100\n",
    "\n",
    "def train_model(epochs=3):\n",
    "    total_loss = 0\n",
    "    i = 0\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in trainloader:\n",
    "            i += 1\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logps = model.forward(inputs)\n",
    "            loss = criterion(logps, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in testloader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    logps = model.forward(inputs)\n",
    "                    batch_loss = criterion(logps, labels)\n",
    "                    test_loss += batch_loss.item()\n",
    "\n",
    "                    ps = torch.exp(logps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            train_losses.append(total_loss/len(trainloader))\n",
    "            test_losses.append(test_loss/len(testloader))                    \n",
    "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                  f\"Train loss: {total_loss/print_every:.3f}.. \"\n",
    "                  f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
    "                  f\"Test accuracy: {accuracy/len(testloader):.3f}\")\n",
    "            running_loss = 0\n",
    "            model.train()\n",
    "    return model\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "model = train_model(epochs=3)\n",
    "torch.save(model, model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, test_losses = [], []\n",
    "model = train_model(epochs=3)\n",
    "print('Finished training, saving model')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "torch.save(model, os.path.join(output_folder, model_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting activity-classification/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $experiment_name/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import tempfile\n",
    "from azureml.core import Dataset, Run\n",
    "import azureml.contrib.dataset\n",
    "from azureml.contrib.dataset import FileHandlingOption, LabeledDatasetTask\n",
    "\n",
    "def load(f, size = .2):\n",
    "    \n",
    "    t = transforms.Compose([transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406], \n",
    "        std = [0.229, 0.224, 0.225])])\n",
    "        \n",
    "    train = datasets.ImageFolder(f, transform=t)\n",
    "    test = datasets.ImageFolder(f, transform=t)\n",
    "    n = len(train)\n",
    "    indices = list(range(n))\n",
    "    split = int(np.floor(size * n))\n",
    "    np.random.shuffle(indices)\n",
    "    train_idx, test_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "    trainloader = torch.utils.data.DataLoader(train,sampler=train_sampler, batch_size=64)\n",
    "    testloader = torch.utils.data.DataLoader(test, sampler=test_sampler, batch_size=64)\n",
    "    return trainloader, testloader\n",
    "\n",
    "def get_mounting_path(labeled_dataset):\n",
    "    \n",
    "    mounted_path = tempfile.mkdtemp()\n",
    "    mount_context = labeled_dataset.mount(mounted_path)\n",
    "    mount_context.start()\n",
    "    print(os.listdir(mounted_path))\n",
    "    print (mounted_path)\n",
    "    print(os.listdir(mounted_path+'/workspaceblobstore'))\n",
    "    return mounted_path + '/workspaceblobstore/activities'\n",
    "\n",
    "def start(output_folder, model_file_name):\n",
    "    \n",
    "    run = Run.get_context()\n",
    "    labeled_dataset = run.input_datasets['activities']\n",
    "    \n",
    "    data_path =  get_mounting_path(labeled_dataset)\n",
    "\n",
    "    trainloader, testloader = load(data_path, .2)\n",
    "    print(trainloader.dataset.classes)\n",
    "    images, labels = next(iter(trainloader))\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = models.resnet18(pretrained=True)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False  \n",
    "\n",
    "    features = model.fc.in_features\n",
    "    model.fc = nn.Linear(features, len(labels))\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    \n",
    "    # train the model\n",
    "    print_every = 100\n",
    "    train_losses, test_losses = [], []\n",
    "    total_loss = 0\n",
    "    i = 0\n",
    "    epochs=3\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in trainloader:\n",
    "            i += 1\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logps = model.forward(inputs)\n",
    "            loss = criterion(logps, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in testloader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    logps = model.forward(inputs)\n",
    "                    batch_loss = criterion(logps, labels)\n",
    "                    test_loss += batch_loss.item()\n",
    "\n",
    "                    ps = torch.exp(logps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            train_losses.append(total_loss/len(trainloader))\n",
    "            test_losses.append(test_loss/len(testloader))                    \n",
    "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                  f\"Train loss: {total_loss/print_every:.3f}.. \"\n",
    "                  f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
    "                  f\"Test accuracy: {accuracy/len(testloader):.3f}\")\n",
    "            running_loss = 0\n",
    "            model.train()\n",
    "    \n",
    "    print('Finished training')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    torch.save(model, os.path.join(output_folder, model_file_name))\n",
    "    print('Model saved:', model_file_name)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--output-folder\", default=None, type=str, dest='output_folder', required=True, help=\"Output folder for the model\")    \n",
    "    parser.add_argument(\"--model-file\", default=None, type=str, dest='model_file_name', required=True, help=\"Output model file\")\n",
    "    args = parser.parse_args()\n",
    "    if args.output_folder:\n",
    "        os.makedirs(args.output_folder, exist_ok=True)\n",
    "    output_folder = args.output_folder\n",
    "    model_file_name = args.model_file_name\n",
    "    print('Output folder:', output_folder)\n",
    "    print('Model file:', model_file_name)\n",
    "    start(output_folder, model_file_name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running experiments in the cloud\n",
    "================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(estimator)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model management\n",
    "================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activities\tactivities:3\t3\n"
     ]
    }
   ],
   "source": [
    "model = run.register_model(model_name='activities', model_path=output_folder+\"/\"+model_file_name)\n",
    "print(model.name, model.id, model.version, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.download_file(name=output_folder+\"/\"+model_file_name, output_file_path='./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
