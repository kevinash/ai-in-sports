{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2 Machine Learning: Automating and Consuming\n",
    "\n",
    "Module 8 - AI in the Cloud\n",
    "\n",
    "For book, references and training materials, please check this project website [http://activefitness.ai/ai-in-sports-with-python](http://activefitness.ai/ai-in-sports-with-python).\n",
    "\n",
    "Book: [Applied Machine Learning for Health and Fitness](https://www.apress.com/us/book/9781484257715), Chapters 11-12\n",
    "\n",
    "## Project: Registering, Deploying and Comsuming Models in the Cloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import azureml.core\n",
    "azureml.core.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "workspace = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial experiment configuration\n",
    "experiment_name = 'activity-classification'\n",
    "script_folder = 'activity-classification'\n",
    "cluster_name = \"pipeline-cluster\"\n",
    "model_file_name = 'activities.pkl'\n",
    "labeled_dataset_name = 'Classifying activities-2020-03-15 00:54:26'\n",
    "output_folder =  'outputs'\n",
    "local_download_folder = './download/' \n",
    "env_name = 'pipeline-env'\n",
    "experiment_name = 'pipeline-experiment'\n",
    "model_name = 'activities'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.core import Workspace\n",
    "\n",
    "auth = InteractiveLoginAuthentication(tenant_id = '72f988bf-86f1-41af-91ab-2d7cd011db47')\n",
    "workspace = Workspace.from_config(auth = auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model activities_classifier\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model = Model.register(model_path = \"./models\",\n",
    "                       model_name = \"activities_classifier\",\n",
    "                       description = \"Activity Classification\",\n",
    "                       tags = {'area':'classification'},\n",
    "                       workspace = workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activities_classifier 3 {'area': 'classification'}\n"
     ]
    }
   ],
   "source": [
    "model = Model(workspace, 'activities_classifier')\n",
    "print(model.name, model.version, model.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'azureml-models\\\\activities_classifier\\\\3\\\\models\\\\activities.pkl'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model.get_model_path('activities_classifier', _workspace=workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a scoring script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting activity-classification/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/score.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import json\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def transform(image_file):\n",
    "    t = transforms.Compose([transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406], \n",
    "        std = [0.229, 0.224, 0.225])])\n",
    "    image = Image.open(image_file)\n",
    "    image = t(image).float()\n",
    "    image = torch.tensor(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "def decode_base64_to_img(base64_string):\n",
    "    base64_image = base64_string.encode('utf-8')\n",
    "    decoded_img = base64.b64decode(base64_image)\n",
    "    return BytesIO(decoded_img)\n",
    "\n",
    "def init():\n",
    "    global model, classes\n",
    "    #model_path = Model.get_model_path('activities')\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'models', 'activities.pkl')\n",
    "    model = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "    model.eval()\n",
    "    #pkl_file = open(os.path.join(model_path,'class_names.pkl'), 'rb')\n",
    "    #classes = pickle.load(pkl_file)\n",
    "    #pkl_file.close() \n",
    "    classes = ['surfing','tennis']\n",
    "\n",
    "def run(input_data):\n",
    "    image = decode_base64_to_img(json.loads(input_data)['data'])\n",
    "    image = transform(image)\n",
    "\n",
    "    output = model(image)\n",
    "\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    pred_probs = softmax(model(image)).detach().numpy()[0]\n",
    "    index = torch.argmax(output, 1)\n",
    "\n",
    "    result = json.dumps({\"label\": classes[index], \"probability\": str(pred_probs[index])})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining an environment\n",
    "=======================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting activity-classification/activities.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/activities.yml\n",
    "name: Activities-PyTorch\n",
    "dependencies:\n",
    "  - python=3.6.2\n",
    "  - pip:\n",
    "    - azureml-defaults\n",
    "    - azureml-core\n",
    "    - azureml-contrib-dataset\n",
    "    - azureml-dataprep[pandas,fuse]\n",
    "    - inference-schema[numpy-support]\n",
    "    - torch\n",
    "    - torchvision\n",
    "    - pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"name\": \"Activities-PyTorch\",\n",
       "    \"version\": \"2\",\n",
       "    \"environmentVariables\": {\n",
       "        \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n",
       "    },\n",
       "    \"python\": {\n",
       "        \"userManagedDependencies\": false,\n",
       "        \"interpreterPath\": \"python\",\n",
       "        \"condaDependenciesFile\": null,\n",
       "        \"baseCondaEnvironment\": null,\n",
       "        \"condaDependencies\": {\n",
       "            \"dependencies\": [\n",
       "                \"python=3.6.2\",\n",
       "                {\n",
       "                    \"pip\": [\n",
       "                        \"azureml-defaults\",\n",
       "                        \"azureml-core\",\n",
       "                        \"azureml-contrib-dataset\",\n",
       "                        \"azureml-dataprep[pandas,fuse]\",\n",
       "                        \"inference-schema[numpy-support]\",\n",
       "                        \"torch\",\n",
       "                        \"torchvision\",\n",
       "                        \"pillow\"\n",
       "                    ]\n",
       "                }\n",
       "            ],\n",
       "            \"name\": \"azureml_29be90bc0029fbe93f78eab2d8a6383f\"\n",
       "        }\n",
       "    },\n",
       "    \"docker\": {\n",
       "        \"enabled\": false,\n",
       "        \"baseImage\": \"mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04\",\n",
       "        \"baseDockerfile\": null,\n",
       "        \"sharedVolumes\": true,\n",
       "        \"shmSize\": null,\n",
       "        \"arguments\": [],\n",
       "        \"baseImageRegistry\": {\n",
       "            \"address\": null,\n",
       "            \"username\": null,\n",
       "            \"password\": null\n",
       "        }\n",
       "    },\n",
       "    \"spark\": {\n",
       "        \"repositories\": [],\n",
       "        \"packages\": [],\n",
       "        \"precachePackages\": true\n",
       "    },\n",
       "    \"databricks\": {\n",
       "        \"mavenLibraries\": [],\n",
       "        \"pypiLibraries\": [],\n",
       "        \"rcranLibraries\": [],\n",
       "        \"jarLibraries\": [],\n",
       "        \"eggLibraries\": []\n",
       "    },\n",
       "    \"r\": null,\n",
       "    \"inferencingStackVersion\": null\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core.environment import Environment\n",
    "\n",
    "env = Environment.from_conda_specification(name='Activities-PyTorch',file_path=script_folder+\"/activities.yml\")\n",
    "env.register(workspace=workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name AzureML-TensorFlow-2.0-CPU\n",
      "packages channels:\n",
      "- conda-forge\n",
      "dependencies:\n",
      "- python=3.6.2\n",
      "- pip:\n",
      "  - azureml-core==1.2.0\n",
      "  - azureml-defaults==1.2.0\n",
      "  - azureml-telemetry==1.2.0\n",
      "  - azureml-train-restclients-hyperdrive==1.2.0\n",
      "  - azureml-train-core==1.2.0\n",
      "  - tensorflow==2.0\n",
      "  - horovod==0.18.1\n",
      "name: azureml_a685c8fa2729bbbf4932e75b8eb0df54\n",
      "\n",
      "Name AzureML-Chainer-5.1.0-GPU\n",
      "packages channels:\n",
      "- conda-forge\n",
      "dependencies:\n",
      "- python=3.6.2\n",
      "- pip:\n",
      "  - azureml-core==1.2.0\n",
      "  - azureml-defaults==1.2.0\n",
      "  - azureml-telemetry==1.2.0\n",
      "  - azureml-train-restclients-hyperdrive==1.2.0\n",
      "  - azureml-train-core==1.2.0\n",
      "  - chainer==5.1.0\n",
      "  - cupy-cuda90==5.1.0\n",
      "  - mpi4py==3.0.0\n",
      "name: azureml_43ae3494b9b7666919116b4a25139bcf\n",
      "\n",
      "Name AzureML-VowpalWabbit-8.8.0\n",
      "packages channels:\n",
      "- conda-forge\n",
      "dependencies:\n",
      "- python=3.6.2\n",
      "- pip:\n",
      "  - azureml-core==1.2.0\n",
      "  - azureml-defaults==1.2.0\n",
      "  - azureml-dataprep[fuse,pandas]\n",
      "name: azureml_eed6129d1cdd3d18a4f0f2b746ad4d83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List environments availablel in the cloud\n",
    "envs = Environment.list(workspace=workspace)\n",
    "for env in list(envs)[0:4]:\n",
    "    if env.startswith(\"AzureML\"):\n",
    "        print(\"Name\",env)\n",
    "        print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploying models\n",
    "================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.model import InferenceConfig, Model\n",
    "from azureml.core.webservice import LocalWebservice\n",
    "\n",
    "def deploy_locally(model_name, port):\n",
    "    model = Model(workspace, model_name)\n",
    "    myenv = Environment.from_conda_specification(name=\"env\", file_path=script_folder+\"/activities.yml\")\n",
    "    inference_config = InferenceConfig(entry_script=script_folder+\"/score.py\", environment=myenv)\n",
    "    deployment_config = LocalWebservice.deploy_configuration(port=port)\n",
    "    return Model.deploy(workspace, model_name, [model], inference_config, deployment_config)\n",
    "\n",
    "service = deploy_locally('activities', 8891)\n",
    "service.wait_for_deployment(True)\n",
    "print(service.port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "from azureml.core.model import Model\n",
    "\n",
    "service_name = 'activity-classification'\n",
    "model = Model(workspace, 'activities')\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)\n",
    "service = Model.deploy(workspace, service_name, [model], inference_config, deployment_config)\n",
    "service.wait_for_deployment(show_output = True)\n",
    "print(service.state)\n",
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "\n",
    "image_path = 'download/workspaceblobstore/activities/surfing/resize-DSC04631.JPG'\n",
    "\n",
    "with open(image_path, 'rb') as file:\n",
    "    byte_content = file.read()\n",
    "    \n",
    "base64_bytes = base64.b64encode(byte_content)\n",
    "base64_string = base64_bytes.decode('utf-8')\n",
    "request = json.dumps({'data': base64_string })\n",
    "prediction = service.run(input_data=request)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model inference URI: \", service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a continuous model training pipeline\n",
    "\n",
    "### Runtime environment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset, Environment\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Create environment and runtime configuration\n",
    "environment = Environment(env_name)\n",
    "environment.docker.enabled = True\n",
    "environment.python.conda_dependencies = CondaDependencies.create(pip_packages=['azureml-sdk',\n",
    "                                                                        'azureml-contrib-dataset',\n",
    "                                                                        'torch','torchvision',\n",
    "                                                                        'azureml-dataprep[pandas,fuse]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2020-03-24T16:59:21.234000+00:00', 'errors': None, 'creationTime': '2020-03-24T13:33:40.051086+00:00', 'modifiedTime': '2020-03-24T13:33:56.048041+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_D3_V2'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=workspace, name=cluster_name)\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RunConfiguration()\n",
    "config.target = compute_target\n",
    "config.environment = environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.get_by_name(workspace, labeled_dataset_name)\n",
    "model_folder = PipelineData(\"model_folder\", datastore=workspace.get_default_datastore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training step \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator(source_directory=script_folder, \n",
    "                      compute_target=compute_target,\n",
    "                      environment_definition=config.environment,\n",
    "                      entry_script='train.py')\n",
    "\n",
    "train_step = EstimatorStep(name = \"Train Model Step\",\n",
    "                           estimator=estimator, \n",
    "                           estimator_entry_script_arguments=['--output-folder', model_folder, '--model-file', model_file_name],\n",
    "                           outputs=[model_folder],\n",
    "                           compute_target=compute_target,\n",
    "                           inputs=[dataset.as_named_input('activities')],\n",
    "                           allow_reuse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting activity-classification/register_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/register_model.py\n",
    "import argparse\n",
    "from azureml.core import Workspace, Model, Run\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_name', type=str, dest='model_name', default=\"activities\", help='Model name.')\n",
    "parser.add_argument('--model_folder', type=str, dest='model_folder', default=\"outputs\", help='Model folder.')\n",
    "parser.add_argument('--model_file', type=str, dest='model_file', default=\"activities.pkl\", help='Model file.')\n",
    "args = parser.parse_args()\n",
    "model_name = args.model_name\n",
    "model_folder = args.model_folder\n",
    "model_file = args.model_file\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "print(\"Model folder:\",model_folder)\n",
    "print(\"Model file:\",model_file)\n",
    "\n",
    "Model.register(workspace=run.experiment.workspace,\n",
    "               model_name = model_name,\n",
    "               model_path = model_folder+\"/\"+model_file)\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining deployment step\n",
    "------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_step = PythonScriptStep(name = \"Register Model Step\",\n",
    "                                source_directory = script_folder,\n",
    "                                script_name = \"register_model.py\",\n",
    "                                arguments = ['--model_name', model_name, '--model_folder', model_folder, '--model_file', model_file_name],\n",
    "                                inputs=[model_folder],\n",
    "                                compute_target = compute_target,\n",
    "                                runconfig = config,\n",
    "                                allow_reuse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps created\n",
      "Pipeline created\n"
     ]
    }
   ],
   "source": [
    "steps = [train_step, register_step]\n",
    "print(\"Steps created\")\n",
    "\n",
    "pipeline = Pipeline(workspace=workspace, steps=steps)\n",
    "print (\"Pipeline created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the pipeline\n",
    "--------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "pipeline_run = Experiment(workspace, experiment_name).submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
